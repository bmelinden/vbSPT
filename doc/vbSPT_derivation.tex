We now derive the core elements of the variational algorithm for
single trajectories, following earlier Variational Bayesian/Ensemble
learning treatments of HMMs\cite{Bronson2010,Mackay1997,Beal2003}.  To
start with, we will feed the model,
Eqs. (\ref{eq:efactor1}-\ref{xmodel1}),
\begin{multline}
\tag{\ref{eq:efactor1}}
p(\x_{1:T},s_{1:T-1},\vec{\gamma},\matris{A},\vec\pi|N)\\
=p(\x_{1:T}|s_{1:T-1},\vec{\gamma})
p(s_{1:T-1}|\matris{A},\vec\pi)p(\vec{\gamma}|N)p(\matris{A}|N)p(\vec\pi|N),
\end{multline}
\begin{align}
\tag{\ref{smodel1}}
  p(s_{1:T-1}|\matris{A},\vec{\pi})=&
  \prod_{m=1}^N\pi_m^{\delta_{m,s_1}}
  \prod_{t=1}^{T-1}\prod_{k,j=1}^N A_{kj}^{\delta_{k,s_t}\delta_{j,s_{t+1}}},\\
    \tag{\ref{xmodel1}}
    p(\x_{1:T}|s_{1:T-1},\vec\gamma)=&
    \prod_{t=1}^{T-1}\prod_{k=1}^N\left(
    \frac{\gamma_k}{\pi}\right)^{\frac d2\delta_{k,s_t}}
    e^{-\delta_{k,s_t}\gamma_k(\x_{t+1}-\x_t)^2}. 
\end{align}
into the mean-field machinery of Eqs.~\eqref{eq:update1}
and \eqref{eq:update2} (with $\theta=\vec{\gamma},\matris{A},\vec{\pi}$,),
\begin{align}
  \ln q(\theta)=&-\ln Z_\theta+\ln p(\theta|N)
  +\mean{\ln p(x,s|\theta,N)}_{q(s)},\tag{\ref{eq:update1}}\\
  \ln q(s)=&-\ln Z_s+\mean{\ln p(x,s|\theta,N)}_{q(\theta)}.
\tag{\ref{eq:update2}}
\end{align}
which also includes choosing functional forms for the prior
distributions that make the averages tractable. In the next
subsections, we first derive the parameter distributions and the
update rule that constitute the M-step of the EM iterations, then go on
to the distribution of hidden states and its update rule (the E-step),
and finally describe how to compute the lower bound $F$ after a
completed E-step.

\subsection{Parameters}
We have three types of parameters,
$\theta=(\vec\pi,\matris{A},\vec \gamma)$, and it will turn out that
if we choose priors that factorize in a clever way,
\begin{equation}\label{pfactorization}
p(\theta|N)=p(\vec{\gamma},\vec{\pi},\matris{A}|N)=p(\vec\pi|N)\prod_jp(\gamma_j|N)p(A_{j,:}|N),
\end{equation}
where $A_{j,:}$ means row $j$ of $\matris{A}$, then the variational
distributions factorize in the same way, which simplifies the
computations. Hence, if we substitute factorized priors of this form,
plus Eqs.~\eqref{smodel1} and \eqref{xmodel1}, into the parameter
distribution equation, Eq.~\eqref{eq:update1}, we get the following
variational distribution for the parameters:
\begin{multline}\label{Maverage}
  \ln q(\vec{\pi},\matris{A},\vec{\gamma})=
  -\ln Z_\theta  
    +\ln p(\matris{A}|N)+\ln p(\vec{\pi}|N)
    +\mean{\ln p(s_{1:T-1}|\matris{A},\vec{\pi)}}_{q(s_{1:T-1})}\\
  +\ln p(\gamma|N)+\mean{\ln p(\x_{1:T}|s_{1:T-1},\vec{\gamma})}_{q(s_{1:T-1})}\\
  =-\ln Z +\sum_{j=1}^N\Big[\mean{\delta_{j,s_1}}_{q(s_{1:T-1})}\ln\pi_j+\ln p(\pi_j)\Big]\\
  +\sum_{j=1}^N\sum_{t=1}^{T-2}\bigg(
    \ln p(A_{j,:})+\sum_{k=1}^N\mean{\delta_{j,s_t}\delta_{k,s_{t+1}}}_{q(s_{1:T-1})}\ln A_{j,k}
    \bigg)\\%  -\ln p(\x_1)\\
  +\sum_{j=1}^N\Bigg[\sum_{t=1}^{T-1}
  \bigg(
  \frac d2\mean{\delta_{j,s_t}}_{q(s_{1:T-1})}\ln\frac{\gamma_j}{\pi}
  -\mean{\delta_{j,s_t}}_{q(s_{1:T-1})}(\x_{t+1}-\x_t)^2\gamma_j
  \bigg)+\ln p(\gamma_j)\Bigg].
\end{multline}
From this, we can read out the functional form of the variational
distribution, which factorize in the same was as the prior. However, we must
still choose prior distributions. We adopt a particularly simple form
known as conjugate priors\cite{Mackay1997,Beal2003}, which enables
a simple interpretation of the prior distribution in terms of
fictitious 'pseudo'-observations.

\paragraph{Initial state and transition rates:}
Following earlier work\cite{Mackay1997,Beal2003,Bronson2009}, we
choose Dirichlet (a multivariate version of the $\beta$ distribution) priors for the initial state distribution and for
each row of the transition rate matrix\cite{wiki:dirichlet}. This
makes the variational distributions Dirichlet distributions as well
(the Dirichlet distribution is its own conjugate). The Dirichlet
density function, in this case for $\vec\pi$, is
\begin{equation}\label{eq:p0pi}
        q(\vec\pi)=\Dir(\vec\pi|\vec{w}^{(\vec{\pi})})=\frac{1}{B(\vec w^{(\vec\pi)})}\prod_j \pi_j^{(w_j^{(\vec\pi)}-1)},
\end{equation}
with the constraints $0\le \pi_j\le1$ and $\sum_j\pi_j=1$, and
normalization constant $B(\vec
w^{(\vec\pi)})=\prod_j\Gamma(w_j^{(\vec\pi)})/\Gamma(\sum_kw_k^{(\vec\pi)})$\cite{wiki:dirichlet}. Inspection
of Eq.~\eqref{Maverage} reveals that
\begin{align}\label{VBM_pi}
  q(\vec{\pi})=&\Dir(\vec{\pi}|\vec{w}^{(\vec{\pi})}),&
  w_j^{(\vec{\pi})}=&\tilde w_j^{(\vec{\pi})}+\mean{\delta_{j,s_1}}_{q(s)}
\\
\label{VBM_A}
  q(\matris{A})=&\prod_j
  \Dir(A_{j,:}|w_{j,:}^{(\matris{A})}),&
  w_{jk}^{(\matris{A})}=&\tilde w_{jk}^{(\matris{A})}
  +\sum_{t=1}^{T-2}\mean{\delta_{j,s_t}\delta_{k,s_{t+1}}}_{q(s)},
\end{align}
where $\tilde w_j^{(\vec{\pi})}$ and $\tilde w_{jk}^{(\matris{A})}$
are the Dirichlet parameters of the prior distributions. Note how the
prior parameters simply add to the data-dependent terms. This means
that the prior distributions can be understood in terms of
'pseudo-counts', imaginary observations prior to seeing the data in
question. The total number of counts (for each distribution) is called
the prior strength.

The following averages\cite{Beal2003}: will turn out to be useful:
\begin{align}
  \mean{\ln \pi_i}_{q(\vec{\pi})}=&\psi(w_i^{(\vec{\pi})})-\psi(w_0^{(\vec{\pi})}),
  &w_0^{(\vec{\pi})}&=\sum_{i=1}^Nw_i^{(\vec{\pi})},\label{lnpiaverage}\\
  \mean{\ln A_{kj}}_{q(\matris{A})}=&\psi(w_{kj}^{(\matris{A})})
    -\psi(w_{k0}^{(\matris{A})}),
  &w_{k0}^{(\matris{A})}=&\sum_{j=1}^Nw_{kj}^{(\matris{A})},\\
  \pi^*_{i\;q(\vec\pi)}=&\frac{w_i^{(\vec\pi)}-1}{w_0^{(\vec\pi)}-N}, &
  A^*_{jk\;q(\matris{A})}=&\frac{w_{jk}^{(\matris{A})}-1}{w_0^{(\matris{A})}-N},\label{Dirmode}\\
  \mean{\pi_i}_{q(\vec{\pi})}=&\frac{w_i^{(\vec\pi)}}{w_0^{(\vec\pi)}}, &
    \mean{A_{jk}}_{q(\matris{A})}=&\frac{w_{jk}^{(\matris{A})}}{w_0^{(\matris{A})}},\label{Dirmean}\\
  \Var[\pi_i]_{q(\vec{\pi})}=&
  \frac{w_i^{(\vec\pi)}(w_0^{(\vec\pi)}-w_i^{(\vec\pi)})}{
    (w_0^{(\vec\pi)})^2(w_0^{(\vec\pi)}+1)},&
    \Var[A_{jk}]_{q(\matris{A})}=&\frac{w_{jk}^{(\matris{A})}(w_0^{(\matris{A})}
      -w_{jk}^{(\matris{A})})}{
    (w_0^{(\matris{A})})^2(w_0^{(\matris{A})}+1)}.\label{Dirvar}
\end{align}
Here, $\psi$ is the digamma function, and $\theta^*_{q(\cdot)}$
denotes the mode (point of maximum density) of the parameter $\theta$
in the distribution $q(\cdot)$.

\paragraph{Diffusion constants:}
The terms involving $\gamma_j$ in \Eq{Maverage} are
\begin{equation}
  \ln q(\gamma_j)=\text{const.}
  +\ln p(\gamma_j|N)+\sum_{t=1}^{T-1}
  \frac d2\mean{\delta_{j,s_t}}\ln\frac{\gamma_j}{\pi}
  -\mean{\delta_{j,s_t}}(\x_{t+1}-\x_t)^2\gamma_j.
\end{equation}
Hence, if we choose the prior to be a gamma-distribution, then the
variational distribution will be so as well, and we get
\begin{equation}
  q(\gamma_j)=\frac{c_j^{n_j}}{\Gamma(n_j)}\gamma_j^{n_j-1}e^{-c_j\gamma_j},
\end{equation}
with
\begin{equation}\label{VBM_gamma}
  n_j=\tilde n_j+\frac d2\sum_{t=1}^{T-1}\mean{\delta_{j,s_t}},\quad
  c_j=\tilde c_j+\sum_{t=1}^{T-1}\mean{\delta_{j,s_t}}(\x_{t+1}-\x_t)^2,
\end{equation}
and $\tilde n_j$, $\tilde c_j$ being prior parameters. Again, we will
need some averages with respect to $q(\gamma_j)$:
\begin{equation}\label{gammaprop}
  \mean{\gamma_j}_{q(\gamma_j)}=\frac{n_j}{c_j},\quad
  \mean{\ln\gamma_j}_{q(\gamma_j)}=\psi(n_j)-\ln c_j,\quad  
  \Var[\gamma_j]_{q(\gamma_j)}=\frac{n_j}{c_j^2},
\end{equation}
and we can also transform back to the diffusion constant
$D_j=(4\gamma_j\Delta t)^{-1}$. Using the rules of probability theory,
the distribution of $D_j$ is inverse gamma,
\begin{equation}\label{eq:qD}
  q(D_j)=\frac{\beta_j^{n_j}}{\Gamma(n_j)}D_j^{-(n_j+1)}e^{-\beta_j/D_j}.
  \quad \beta_j=c_j/(4\Delta t),
\end{equation}
This means that
\begin{align}
  D^*_{j\;q(D_j)}=&\frac{c}{4(n_j+1)\Delta t},\\
  \mean{D_j}_{q(D_j)}=&\frac{c}{4(n_j-1)\Delta t},\\
  \std[D_j]_{q(D_j)} =&\frac{\mean{D_j}}{\sqrt{n_j-2}}.
\end{align}
The mean value and standard deviation are only defined if $n_i>1,2$
respectively.

The M-step of the iterations, Eq.~\eqref{eq:update1}, thus consists of
updating the variational parameter distributions according to
equations \eqref{VBM_pi}, \eqref{VBM_A}, and \eqref{VBM_gamma}. These
equations in turn contain certain averages of the hidden state
distribution $q(s_{1:T-1})$. We now go on and derive the variational
distribution for the hidden state and the E-step, which allows us to
compute these averages.
\subsection{Hidden states}
Collecting the terms in Eqs. \eqref{xmodel1}
and \eqref{smodel1} that depend on the hidden states, we get
\begin{multline}\label{VBE_qs}
 \ln q(s_{1:T-1})=
-\ln Z_s+\mean{\ln p(s_{1:T-1}|\matris{A},\vec{\pi)}}_{q(\matris{A})q(\vec\pi)}
+\mean{\ln p(\x_{1:T}|s_{1:T-1},\vec{\gamma})}_{q(\vec\gamma)}\\
=-\ln Z_s +\sum_{t=1}^{T-1}\ln H_{t,s_t}+\sum_{t=1}^{T-2}\ln Q_{s_t,s_{t+1}}.
\end{multline}
This looks a like the Hamiltonian of a 1-dimensional spin model in
statistical physics, with external field $\ln H_{t,j}$ and
nearest-neighbor coupling $\ln Q_{jk}$ given by
\begin{align}
  \ln H_{t,j}=& \delta_{1,t}\underbrace{
    \big[\psi(w_j^{(\vec\pi)})-\psi(w_0^{(\vec\pi)})\big]
  }_{\text{initial state distribution}}
  +\frac d2\big(\psi(n_j)-\ln \pi c_j\big)-\frac{n_j}{c_j}(\x_{t+1}-\x_t)^2,
  \\
  \ln Q_{j,k}=&\psi(w_{jk}^{(\matris{A})})-\psi(w_{j0}^{(\matris{A})}).
\end{align}
For this distribution, we also need a couple of expectation values
to feed back in the next iteration of the parameter
distributions, namely
\begin{equation}
  \mean{\delta_{j,s_t}}_{q(s)}=p(s_t=j),\quad
  \mean{\delta_{j,s_t}\delta_{k,s_{t+1}}}_{q(s)}=p(s_{t+1}=k|s_t=j).
\end{equation}
These can be computed by a dynamic programming
trick\cite{Bishop2006,Mackay1997,Beal2003}, known in the HMM context
as the forward-backward or Baum-Welch
algorithm\cite{Rabiner1989,Baum1972}. As a side effect, one also
obtains the normalization constant $Z_s$, which will be needed below
when computing the lower bound $F$.

\paragraph{Multiple trajectories:}
To analyze many trajectories, one tries to optimize the sum of the
lower bounds for each trajectory with a single model. For the
EM-iterations, this means that the parameters in the variational
distributions get contributions from each trajectory that are just
summed up, \textit{i.e.} averages over the hidden state distribution gets
extended with a summation over $M$ trajectories of length $T_m$ as
well, \textit{e.g.} in Eq.~\eqref{VBM_A},
\begin{equation}
w_{jk}^{(\matris{A})}=\tilde w_{jk}^{(\matris{A})}
  +\sum_{m=1}^M\sum_{t=1}^{T_m-2}\mean{\delta_{j,s^m_t}\delta_{k,s^m_{t+1}}}_{q(s)}.
\end{equation}
We get a variational distribution over the hidden states in each
trajectory, where the coupling constants $Q_{jk}$ and external fields
$H_{tj}$ are given by the same parameter distributions (but differ in
the contributions from data).


\subsection{Lower bound}
The lower bound $F$ can be computed just after the
E-step. Substituting our variational ansatz
$q(\theta,s)=q(\theta)q(s)$ into our general expression for $F$,
Eq.~\eqref{eq:F1}, we can exploit the fact that $q(s)$ and $q(\theta)$
are normalized probability distributions, and rewrite it in the form
\begin{multline}
F[q(\theta)q(s),x]=\int d\theta\sum_s \Big[ q(\theta)q(s) \ln
    p(x,s|\theta,N)p(\theta|N) -q(\theta)q(s) \ln q(\theta)q(s) \Big]\\
=\sum_sq(s)\Big[\mean{\ln p(x,s|\theta,N)}_{q(\theta)} -\ln q(s)\Big]
+\int d\theta q(\theta)\Big[\ln p(\theta|N)-\ln q(\theta)\Big].
\end{multline}
Just after the E-step however, $q(s)$ is given by
Eq.~\eqref{eq:update2}, which substituted in the above expressions
leads to cancellations, leaving us with
\begin{equation}
F[q(\theta)q(s),x]=\ln Z_s-\int d\theta q(\theta)\ln\frac{q(\theta)}{p(\theta|N)}.
\end{equation}
The first term is just the normalization constant in
Eq.~\eqref{eq:update2}, which comes out as a by-product of the
forward-backward algorithm. The second term is the negative
Kullback-Leibler divergence of the variational parameter distribution
with respect to the prior. In our case, this distribution factorizes
as in Eq.~\eqref{pfactorization}, and we get a sum of separate and
tractable contributions,
\begin{multline}
\int d\theta q(\theta)\ln\frac{q(\theta)}{p(\theta|N)}
     =\int d\vec\pi q(\vec\pi)\ln\frac{q(\vec\pi)}{p(\vec\pi|N)}\\
     +\sum_j\int dA_{j,:} q(A_{j,:})\ln\frac{q(A_{j,:})}{p(A_{j,:}|N)}
     +\sum_j\int d\gamma_j q(\gamma_j)\ln\frac{q(\gamma_j)}{p(\gamma_j|N)}.
\end{multline}
For the initial state distribution, we get
\begin{multline}
  \int d\vec{\pi} q(\vec{\pi}) \ln\frac{q(\vec{\pi})}{p(\vec{\pi}|N)}
  =\mean{\ln\left(\frac{\Gamma(w_0^{(\vec{\pi})})}{\Gamma(\tilde w_0^{(\vec{\pi})})}
    \prod_{j=1}^N\frac{\Gamma(\tilde w_j^{(\vec{\pi})})}{\Gamma(w_j^{(\vec{\pi})})}
    \pi_j^{w_j^{(\vec{\pi})}-\tilde w_j^{(\vec{\pi})}}\right)}_{q(\vec{\pi})}\\
  =\ln\frac{\Gamma(w_0^{(\vec{\pi})})}{\Gamma(\tilde w_0^{(\vec{\pi})})}
  +\sum_{j=1}^N\left[\ln\frac{\Gamma(\tilde w_j^{(\vec{\pi})})}{\Gamma(w_j^{(\vec{\pi})})}
    +(w_j^{(\vec{\pi})}-\tilde w_j^{(\vec{\pi})})
    \big(\psi(w_j^{(\vec{\pi})})-\psi(w_0^{(\vec{\pi})})\big)\right],
\end{multline}
where we used Eq.~\eqref{lnpiaverage} for
$\mean{\ln \pi_i}_{q(\vec{\pi})}$. The terms from the transition
matrix rows are also Dirichlet distributed, and come out as
\begin{multline}
  \int dA_{j,:} q(A_{j,:}) \ln\frac{q(A_{j,:})}{p(A_{j,:}|N)}
  =\mean{\ln\left(\frac{\Gamma(w_{j0}^{(\matris{A})})}{\Gamma(\tilde w_{j0}^{(\matris{A})})}
    \prod_{k=1}^N\frac{\Gamma(\tilde w_{jk}^{(\matris{A})})}{\Gamma(w_{jk}^{(\matris{A})})}
    A_{jk}^{w_{jk}^{(\matris{A})}-\tilde w_{jk}^{(\matris{A})}}\right)}_{q(\matris{A})}\\
  =\ln\frac{\Gamma(w_{j0}^{(\matris{A})})}{\Gamma(\tilde w_{j0}^{(\matris{A})})}
  +\sum_{k=1}^N\left[\ln\frac{\Gamma(\tilde w_{jk}^{(\matris{A})})}{\Gamma(w_{jk}^{(\matris{A})})}
    +(w_{jk}^{(\matris{A})}-\tilde w_{jk}^{(\matris{A})})
    \big(\psi(w_{jk}^{(\matris{A})})-\psi(w_{j0}^{(\matris{A})})\big)
\right].
\end{multline}
Finally, the terms for the precision parameter $\gamma_j$ can be
written in the form
\begin{multline}
\int d\gamma_j q(\gamma_j)\ln\frac{q(\gamma_j)}{p(\gamma_j|N)}
=\mean{\ln\frac{c_j^{n_j}\gamma_j^{n_j-1}e^{-c_j\gamma_j}\Gamma(\tilde n_j)}
      {\tilde c_j^{\tilde n_j}\gamma_j^{\tilde n_j-1}e^{-\tilde c_j\gamma_j}\Gamma(n_j)}}_{q(\gamma_j)}\\
=n_j\ln\frac{c_j}{\tilde c_j}                    
-\ln\frac{\Gamma(n_j)}{\Gamma(\tilde n_j)}
+(n_j-\tilde n_j)\psi(n_j)
-n_j\Big(1-\frac{\tilde c_j}{c_j}\Big),
\end{multline}
where in the last step, we substituted Eq.~\eqref{gammaprop} for
$\mean{\gamma_j}_{q(\gamma_j)}$ and
$\mean{\ln\gamma_j}_{q(\gamma_j)}$.


\subsection{Matlab notation}
\label{Sec:matlabNotation}
For future reference, we end by listing a translation table between
the notation used in this derivation and the variable names used in
the VB3 code, with the matlab model object named \texttt{W}.

\begin{itemize}
\item The parameters of the parameter variational distributions are
  collected in the \texttt{W.M} and \texttt{W.PM} fields. These are the only fields needed
  to start iterating, all the rest are computed by the algorithm. If
  the \texttt{W.E} field is present, then the \texttt{W.M} field is overwritten in the first
  iteration (and hence the \texttt{W.E} field must be deleted if one wants to use
  the \texttt{W.M} field to parametrize an initial guess).
\begin{center}\begin{tabular}{r|l|r|l|c}
  \textbf{Matlab VB3}& \textbf{This note} & \textbf{Matlab VB3} & \textbf{This note} & \textbf{Eq.} \\
  \hline
\ST  \verb+W.M.wPi(i)+ & $w^{(\vec\pi)}_i$        &
  \verb+W.PM.wPi(i)+& $\tilde w^{(\vec\pi)}_i$ & \eqref{VBM_pi} \\
  \hline
\ST  \verb+W.M.wA(j,k)+ & $w^{(\matris{A})}_{jk}$        &
  \verb+W.PM.wA(j,k)+& $\tilde w^{(\matris{A})}_{jk}$ & \eqref{VBM_A}\\
  \hline
  \verb+W.M.n(j)+ & $n_j$ & 
  \verb+W.PM.n(j)+& $\tilde n_j$ & \eqref{VBM_gamma}\\
  \verb+W.M.c(j)+ & $c_j$ & \verb+W.PM.c(j)+ & $\tilde c_j$ & \\
  \hline
\end{tabular}\end{center}

\item Expectation values computed in the E-step are in the \texttt{W.E}
  fields. Each trajectory gets its own field, and for trajectory \texttt{m},
  the notation is
\begin{center}\begin{tabular}{r|l|c}
\textbf{Matlab VB3}& \textbf{This note} & \textbf{Eq.}\\
\hline
\ST  \verb+W.E(m).wPi(j)+ & $\mean{\delta_{j,s_1}}$ & \eqref{VBM_pi} \\
  \hline
\ST  \verb+W.E(m).wA(j,k)+& $\sum_{t=1}^{T-2}\mean{\delta_{j,s_t}\delta_{k,s_{t+1}}}$ & \eqref{VBM_A}\\
  \hline
\ST  \verb+W.E(m).n(j) +  & $\frac d2\sum_{t=1}^{T-1}\mean{\delta_{j,s_t}}$ & \eqref{VBM_gamma}\\
  \hline
\ST  \verb+W.E(m).c(j) +  & $\sum_{t=1}^{T-1}\mean{\delta_{j,s_t}}(\x_{t+1}-\x_t)^2$ & \eqref{VBM_gamma}\\
  \hline
\end{tabular}\end{center}
\item Misc fields:
\begin{center}\begin{tabular}{l|l}
\hline
\verb+W.N+ & Number of hidden states $N$.\\
\hline
\verb+W.F+ & Total lower bound $F$.\\
\hline
\verb+W.Fterms+ & Various contributions to $F$ (for debugging). \\
\hline
\verb+W.T(j)+ & Length of trajectory j. Note that this counts \\
              &the number of steps, and hence trajectory j will\\
              & contain \verb+W.T(j)++1 particle positions.\\
\hline
\end{tabular}\end{center}
\item Interesting estimates are supplied to describe the output and
  make sense of the converged model and the data. They divided into
  two fields, \texttt{W.est} for quantities that are small and cheap to
  compute (and hence are given every time), while \texttt{W.est2}
  contain quantities that are either large or expensive to compute,
  and therefore only supplied when the iterator algorithm is called
  with the \texttt{'estimate'} argument. When given, the index \texttt{m}
  refers to trajectory number.
  
  \begin{center}\begin{tabular}{l|l|c}
      \textbf{Matlab VB3}& \textbf{This note} & \textbf{Eq.}\\
      \hline
\ST      \verb+W.est.lnQ(j,k)+ & $\ln Q_{j,k}$ & \eqref{VBE_qs}\\
      \hline
\ST      \verb+W.est.Q(j,k)+   & $Q_{j,k}/\max_{j,k}(Q_{j,k})$ & \\
      \hline
\ST       \verb+W.est.Ts(j)+& $\sum_t\mean{\delta_{j,s_t}}$ & \\
      \hline
\ST       \verb+W.est.Ps+& \verb+W.est.Ts/sum(W.est.Ts)+ & \\
      \hline
\ST       \verb+W.est.Amean(j,k)+& $\mean{A_{jk}}_{q(\matris(A)}$ & \eqref{Dirmean} \\
      \hline
\ST       \verb+W.est.Amode(j,k)+& $A^*_{jk\;q(\matris{A})}$& \eqref{Dirmode} \\
      \hline
\ST       \verb+W.est.Astd(j,k)+& $\std[A_{jk\;q(\matris{A})}]$ & \eqref{Dirvar} \\
      \hline
\ST       \verb+W.est.lnAmean(k,j)+&$e^{\verb+W.est.lnAmean+}=\mean{\matris{A}}$ & approx. rate matrix [$\Delta t^{-1}$]\\
      \hline
\ST       \verb+W.est.lnAmode(k,j)+&$e^{\verb+W.est.lnAmode+}=\matris{A}^*$ &approx. rate matrix [$\Delta t^{-1}$]\\
      \hline
\ST       \verb+W.est.dwellMean(j)+& $(1-\mean{A_{jj}})^{-1} $ & dwell time \\
      \hline
\ST       \verb+W.est.dwellMode(j)+& $(1-A^*_{jj})^{-1} $ & dwell time \\
      \hline
\ST       \verb+W.est.gMean(j)+& $\mean{\gamma_j}_{q(\vec\gamma)}$ & \eqref{gammaprop} \\
      \hline
\end{tabular}\end{center}
\begin{center}\begin{tabular}{l|l|c}
    \textbf{Matlab VB3}& \textbf{This note} & \textbf{Eq.}\\
    \hline
    \verb+W.est2.lnH{m}(t,j)+&$\ln H_{t,j} $ & \eqref{VBE_qs} \\
    \hline
    \verb+W.est2.H{m}(t,j)+& $H_{t,j}/ \max_k(H_{t,k})$ & \\
    \hline      
    \verb+W.est2.viterbi{m}+& $ $ & Viterbi path, trj. $m$. \\
    \hline
    \verb+W.est2.sMaxP{m}(j)+& $\mathrm{argmax}_{s_t^m}\mean{\delta_{j,s_t^m}} $ & Most likely states.  \\
    \hline
    \verb+W.est.pst{m}(t,j)+& $\mean{\delta_{j,s_t^m}}$ & Occupation probability $p(s_t^m=j)$.\\
\hline
   
\end{tabular}\end{center}
\end{itemize}




